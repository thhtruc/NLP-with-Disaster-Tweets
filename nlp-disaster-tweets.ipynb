{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-21T12:00:12.722549Z","iopub.execute_input":"2023-05-21T12:00:12.723489Z","iopub.status.idle":"2023-05-21T12:00:12.752601Z","shell.execute_reply.started":"2023-05-21T12:00:12.723450Z","shell.execute_reply":"2023-05-21T12:00:12.751528Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\", encoding=\"latin-1\", dtype={'id': np.int16, 'target': np.int8})\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\", encoding=\"latin-1\", dtype={'id': np.int16})\n\ntrain_total_row, _ = train_df.shape\ntest_total_row, _ = test_df.shape\nprint(f\"Training dataset shape is: {train_total_row}\")\nprint(f\"Test dataset shape is is {test_total_row}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:13:13.017216Z","iopub.execute_input":"2023-05-21T15:13:13.017603Z","iopub.status.idle":"2023-05-21T15:13:13.118507Z","shell.execute_reply.started":"2023-05-21T15:13:13.017570Z","shell.execute_reply":"2023-05-21T15:13:13.117649Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Training dataset shape is: 7613\nTest dataset shape is is 3263\n","output_type":"stream"}]},{"cell_type":"code","source":"# Number of missing value\n\nmissed_loc = len(train_df[train_df.location.isnull()])\nmissed_kw = len(train_df[train_df.keyword.isnull()])\nmissed_text = len(train_df[train_df.text.isnull()])\n\nprint(f\"Missing location: {missed_loc}\")\nprint(f\"Missing keywords: {missed_kw}\")\nprint(f\"Missing text: {missed_text}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:17:18.850435Z","iopub.execute_input":"2023-05-21T15:17:18.851065Z","iopub.status.idle":"2023-05-21T15:17:18.871120Z","shell.execute_reply.started":"2023-05-21T15:17:18.851031Z","shell.execute_reply":"2023-05-21T15:17:18.869839Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Missing location: 2533\nMissing keywords: 61\nMissing text: 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Distributions training set","metadata":{}},{"cell_type":"code","source":"num_pos = len(train_df[train_df['target'] == 1])\nnum_neg = len(train_df[train_df['target'] == 0])\n\nprint(f\"Number of positive sample: {num_pos}\")\nprint(f\"Number of negative sample: {num_neg}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:20:28.086641Z","iopub.execute_input":"2023-05-21T15:20:28.087061Z","iopub.status.idle":"2023-05-21T15:20:28.096374Z","shell.execute_reply.started":"2023-05-21T15:20:28.087028Z","shell.execute_reply":"2023-05-21T15:20:28.095323Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Number of positive sample: 3271\nNumber of negative sample: 4342\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Loading dataset","metadata":{}},{"cell_type":"code","source":"class Dictionary:\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\nclass Corpus:\n    def __init__(self, DATA_DIR, df):\n        self.dictionary = Dictionary()\n        self.data = self.tokenize(DATA_DIR, df)\n\n    def tokenize(self, DATA_DIR, df):\n        for row in df:\n            words = row.text.split() + ['<eos>']\n            tokens += len(words)\n            for word in words:\n                self.dictionary.add_word(word)\n\n        # Tokenize file content\n        with open(DATA_DIR, 'r') as f:\n            ids = torch.LongTensor(tokens)\n            token = 0\n            for line in f:\n                words = line.split() + ['<eos>']\n                for word in words:\n                    ids[token] = self.dictionary.word2idx[word]\n                    token += 1\n\n        return ids","metadata":{"execution":{"iopub.status.busy":"2023-05-21T16:33:38.858145Z","iopub.execute_input":"2023-05-21T16:33:38.858565Z","iopub.status.idle":"2023-05-21T16:33:38.869270Z","shell.execute_reply.started":"2023-05-21T16:33:38.858529Z","shell.execute_reply":"2023-05-21T16:33:38.867921Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n\nclass DisasterDataset(Dataset):\n    \"\"\"Loading Disaster Dataset\"\"\"\n    def __init__(self, train_df):\n        self.text = []\n        self.label = []\n        self.location = []\n        self.kw = []\n        for row in train_df:\n            self.text.append(row.text)\n            self.label.append(row.target)\n            self.location.append(row.location)\n            self.kw.append(row.keyword)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return torch.tensor([float(x) for x in self.data[idx]])\n\n    \nbatch_size = 32\ndataset = NLPDataset(train_df)\ntrain_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:54:49.031914Z","iopub.execute_input":"2023-05-21T15:54:49.032371Z","iopub.status.idle":"2023-05-21T15:54:49.042950Z","shell.execute_reply.started":"2023-05-21T15:54:49.032333Z","shell.execute_reply":"2023-05-21T15:54:49.041539Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass LSTMClassifier(nn.ModuleList):\n\n    def __init__(self, batch_size, hidden_dim, lstm_layers, input_size):\n        super(LSTMClassifier, self).__init__()\n        \n        self.batch_size = batch_size\n        self.hidden_dim = hidden_dim\n        self.LSTM_layers = lstm_layers\n        self.input_size = input_size # embedding dimention\n        \n        self.dropout = nn.Dropout(0.5)\n        self.embedding = nn.Embedding(self.input_size, self.hidden_dim)\n        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)\n        self.fc = nn.Linear(in_features=self.hidden_dim, out_features=2)\n\n    def forward(self, sentence):\n        embeds = self.embedding(sentence)\n        x = embeds.view(len(sentence), self.batch_size, -1)\n        lstm_out, self.hidden = self.lstm(x, self.hidden)\n        y  = self.fc(lstm_out[-1])\n        return y","metadata":{},"execution_count":null,"outputs":[]}]}
